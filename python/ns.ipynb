{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатываю файл: A Functional Approach to Java - Ben Weidig[2023].pdf\n",
      "Обрабатываю файл: Core Java. Made Simple - Som Prakash Rai[2023].pdf\n",
      "Обрабатываю файл: Cryptography and Cryptanalysis in Java. Second edition.pdf\n",
      "Обрабатываю файл: Getting Started With Java Using Eclipse - Bernhard Steppan[2023].pdf\n",
      "Обрабатываю файл: Java Memory Management - Maaike van Putten, Sean Kennedy[2022].pdf\n",
      "Обрабатываю файл: Java устранение проблем. Чтение, отладка и оптимизация JVM-приложений 2023.pdf\n",
      "Обрабатываю файл: Java. Задачи по основам программирования.pdf\n",
      "Обрабатываю файл: Java. Полное руководство, 12-е изд.pdf\n",
      "Обрабатываю файл: Java. Руководство для начинающих, 9-е изд. (2023).pdf\n",
      "Обрабатываю файл: Java. Самое полное руководство по разработке  - 2024.pdf\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate output buffer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m     save_tokenizer_and_model(tokenizer, model)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Извлекаем текст и код из PDF\u001b[39;00m\n\u001b[0;32m    161\u001b[0m text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(pdf_path)\n\u001b[1;32m--> 162\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43mextract_code_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Определяем язык текста и кода\u001b[39;00m\n\u001b[0;32m    165\u001b[0m lang_text \u001b[38;5;241m=\u001b[39m detect_language(text)\n",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m, in \u001b[0;36mextract_code_from_pdf\u001b[1;34m(pdf_file)\u001b[0m\n\u001b[0;32m     39\u001b[0m code_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n\u001b[1;32m---> 41\u001b[0m     extracted_text \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extracted_text:\n\u001b[0;32m     43\u001b[0m         lines \u001b[38;5;241m=\u001b[39m extracted_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\page.py:548\u001b[0m, in \u001b[0;36mPage.extract_text\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_textmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtuplify_list_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mas_string\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\page.py:525\u001b[0m, in \u001b[0;36mPage._get_textmap\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m     defaults\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout_height\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight})\n\u001b[0;32m    524\u001b[0m full_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mchars_to_textmap(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\container.py:52\u001b[0m, in \u001b[0;36mContainer.chars\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchars\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_obj_list:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\page.py:357\u001b[0m, in \u001b[0;36mPage.objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects\n\u001b[1;32m--> 357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects: Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\page.py:461\u001b[0m, in \u001b[0;36mPage.parse_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_objects\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list]:\n\u001b[0;32m    460\u001b[0m     objects: Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_layout_objects(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayout\u001b[49m\u001b[38;5;241m.\u001b[39m_objs):\n\u001b[0;32m    462\u001b[0m         kind \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manno\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\page.py:279\u001b[0m, in \u001b[0;36mPage.layout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m device \u001b[38;5;241m=\u001b[39m PDFPageAggregatorWithMarkedContent(\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf\u001b[38;5;241m.\u001b[39mrsrcmgr,\n\u001b[0;32m    275\u001b[0m     pageno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_number,\n\u001b[0;32m    276\u001b[0m     laparams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf\u001b[38;5;241m.\u001b[39mlaparams,\n\u001b[0;32m    277\u001b[0m )\n\u001b[0;32m    278\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m PDFPageInterpreter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf\u001b[38;5;241m.\u001b[39mrsrcmgr, device)\n\u001b[1;32m--> 279\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout: LTPage \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mget_result()\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdfinterp.py:997\u001b[0m, in \u001b[0;36mPDFPageInterpreter.process_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    995\u001b[0m     ctm \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39mx0, \u001b[38;5;241m-\u001b[39my0)\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mbegin_page(page, ctm)\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mend_page(page)\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdfinterp.py:1016\u001b[0m, in \u001b[0;36mPDFPageInterpreter.render_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_resources(resources)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state(ctm)\n\u001b[1;32m-> 1016\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdfinterp.py:1021\u001b[0m, in \u001b[0;36mPDFPageInterpreter.execute\u001b[1;34m(self, streams)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, streams: Sequence[\u001b[38;5;28mobject\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1021\u001b[0m         parser \u001b[38;5;241m=\u001b[39m \u001b[43mPDFContentParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m PSEOF:\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;66;03m# empty page\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdfinterp.py:251\u001b[0m, in \u001b[0;36mPDFContentParser.__init__\u001b[1;34m(self, streams)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mistream \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# PSStackParser.__init__(fp=None) is safe only because we've overloaded\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# all the methods that would attempt to access self.fp without first\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calling self.fillfp().\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mPSStackParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\psparser.py:545\u001b[0m, in \u001b[0;36mPSStackParser.__init__\u001b[1;34m(self, fp)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fp: BinaryIO) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     \u001b[43mPSBaseParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\psparser.py:193\u001b[0m, in \u001b[0;36mPSBaseParser.__init__\u001b[1;34m(self, fp)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fp: BinaryIO) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdfinterp.py:263\u001b[0m, in \u001b[0;36mPDFContentParser.seek\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mseek\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillfp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     PSStackParser\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m, pos)\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdfinterp.py:260\u001b[0m, in \u001b[0;36mPDFContentParser.fillfp\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PSEOF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected EOF, file truncated?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m BytesIO(\u001b[43mstrm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdftypes.py:396\u001b[0m, in \u001b[0;36mPDFStream.get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 396\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdftypes.py:336\u001b[0m, in \u001b[0;36mPDFStream.decode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m LITERALS_FLATE_DECODE:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# will get errors if the document is encrypted.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mzlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m zlib\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mSTRICT:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate output buffer."
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import re\n",
    "import sqlite3\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DetectorFactory.seed = 0  \n",
    "pdf_folder = \"G:/swer/my_pdfs/\"\n",
    "output_folder = \"G:/swer/output/\"\n",
    "db_path = \"G:/swer/dataset.db\"\n",
    "tokenizer_path = \"G:/swer/tokenizer.pkl\"\n",
    "model_path = \"G:/swer/code_generation_model.h5\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    doc = fitz.open(pdf_file)\n",
    "    text = \" \".join([page.get_text(\"text\") for page in doc])\n",
    "    return text.strip()\n",
    "\n",
    "def process_pdf_folder(pdf_folder):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            print(f\"Обрабатываю файл: {filename}\")\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            all_texts.append(text)\n",
    "    return all_texts\n",
    "\n",
    "def extract_code_from_pdf(pdf_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        code_blocks = []\n",
    "        for page in pdf.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                lines = extracted_text.split(\"\\n\")\n",
    "                code_lines = [line for line in lines if re.search(r'^\\s*(def |class |import |\\(|\\)|{|}|;|#)', line)]\n",
    "                if code_lines:\n",
    "                    code_blocks.append(\"\\n\".join(code_lines))\n",
    "    return \"\\n\\n\".join(code_blocks).strip()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip(), detect_language(text)\n",
    "\n",
    "def create_database(db_path=\"dataset.db\"):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dataset (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            text TEXT,\n",
    "            code TEXT,\n",
    "            lang_text TEXT,\n",
    "            lang_code TEXT,\n",
    "            source TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_lang_text ON dataset(lang_text)\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_lang_code ON dataset(lang_code)\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_data(text, code, lang_text, lang_code, source, db_path=\"dataset.db\"):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO dataset (text, code, lang_text, lang_code, source) \n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", (text, code, lang_text, lang_code, source))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def tokenize_data(db_path=\"dataset.db\", max_words=10000, max_len=512):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT text, code FROM dataset\")\n",
    "    data = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    texts = [row[0] for row in data]\n",
    "    codes = [row[1] for row in data]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts + codes)\n",
    "\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    code_sequences = tokenizer.texts_to_sequences(codes)\n",
    "\n",
    "    text_data = pad_sequences(text_sequences, maxlen=max_len, padding=\"post\")\n",
    "    code_data = pad_sequences(code_sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "    with open(tokenizer_path, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    return text_data, code_data, tokenizer.word_index\n",
    "\n",
    "def save_tokenizer_and_model(tokenizer, model):\n",
    "    \"\"\"Сохраняет токенизатор и модель.\"\"\"\n",
    "    with open(tokenizer_path, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    model.save(model_path)\n",
    "    print(\"Токенизатор и модель сохранены.\")\n",
    "\n",
    "def prepare_datasets(text_data, code_data, batch_size=32, test_size=0.2):\n",
    "    \"\"\"Готовит tf.data.Dataset для обучения модели.\"\"\"\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(text_data, code_data, test_size=test_size)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def build_model(vocab_size, embedding_dim=128, lstm_units=256):\n",
    "    \"\"\"Создаёт модель для генерации кода.\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        LSTM(lstm_units),\n",
    "        Dense(vocab_size, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model  \n",
    "\n",
    "def train_model(train_dataset, test_dataset, vocab_size, epochs=10):\n",
    "    \"\"\"Обучает модель и сохраняет её.\"\"\"\n",
    "    model = build_model(vocab_size)\n",
    "    model.fit(train_dataset, validation_data=test_dataset, epochs=epochs)\n",
    "    \n",
    "    # Сохранение модели\n",
    "    model.save(model_path)\n",
    "    print(\"Модель сохранена как code_generation_model.h5\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    create_database()\n",
    "\n",
    "    # Обрабатываем все файлы из папки pdf_folder\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        print(f\"Обрабатываю файл: {pdf_file}\")\n",
    "        \n",
    "        # Извлекаем текст и код из PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        code = extract_code_from_pdf(pdf_path)\n",
    "        \n",
    "        # Определяем язык текста и кода\n",
    "        lang_text = detect_language(text)\n",
    "        lang_code = detect_language(code)\n",
    "        \n",
    "        # Вставляем данные в базу данных\n",
    "        insert_data(text, code, lang_text, lang_code, pdf_file)\n",
    "\n",
    "    # Токенизация данных\n",
    "    text_data, code_data, word_index = tokenize_data()\n",
    "\n",
    "    # Подготовка датасетов для обучения\n",
    "    train_dataset, test_dataset = prepare_datasets(text_data, code_data)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(train_dataset, test_dataset, vocab_size=len(word_index))\n",
    "\n",
    "    # Сохраняем токенизатор и модель\n",
    "    save_tokenizer_and_model(tokenizer, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
