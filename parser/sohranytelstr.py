import os
import re
import sqlite3
import requests
import pdfkit
import pandas as pd
from bs4 import BeautifulSoup
from time import sleep
from markdown import markdown

# –ë–∞–∑–æ–≤—ã–π URL
BASE_URL = "https://javarush.com"
ARTICLES_URL = BASE_URL + "/all-articles"

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
SAVE_DIR = "JavaRush_Articles"
IMG_DIR = os.path.join(SAVE_DIR, "images")
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ SQLite
DB_PATH = os.path.join(SAVE_DIR, "articles.db")
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
cursor.execute("""
CREATE TABLE IF NOT EXISTS articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,
    url TEXT UNIQUE,
    category TEXT,
    saved_at TEXT
)
""")
conn.commit()

# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏
def get_article_links():
    links = set()
    page = 1
    while True:
        url = f"{ARTICLES_URL}?page={page}"
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}...")

        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        if response.status_code != 200:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥.")
            break

        soup = BeautifulSoup(response.text, "html.parser")
        new_links = {
            BASE_URL + a["href"]
            for a in soup.find_all("a", href=True)
            if "/groups/posts/" in a["href"]
        }

        if not new_links:
            break

        links.update(new_links)
        page += 1
        sleep(1)

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Ç–∞—Ç–µ–π.")
    return links

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏
def save_article(article_url):
    response = requests.get(article_url, headers={"User-Agent": "Mozilla/5.0"})
    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ {response.status_code}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
    title_tag = soup.find("h1")
    title = title_tag.text.strip() if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    filename_base = re.sub(r"[^\w\-_]", "_", title)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    category = "–†–∞–∑–Ω–æ–µ"
    for cat in ["Java", "Python", "–ê–ª–≥–æ—Ä–∏—Ç–º—ã"]:
        if cat.lower() in title.lower():
            category = cat
            break

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î (–ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è —Å—Ç–∞—Ç—å—è)
    cursor.execute("SELECT id FROM articles WHERE url=?", (article_url,))
    if cursor.fetchone():
        print(f"–°—Ç–∞—Ç—å—è '{title}' —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return None

    cursor.execute("INSERT INTO articles (title, url, category, saved_at) VALUES (?, ?, ?, datetime('now'))",
                   (title, article_url, category))
    conn.commit()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML
    html_path = os.path.join(SAVE_DIR, f"{filename_base}.html")
    with open(html_path, "w", encoding="utf-8") as file:
        file.write(response.text)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Markdown
    md_content = f"# {title}\n\n{article_url}\n\n" + soup.find("article").text
    md_path = os.path.join(SAVE_DIR, f"{filename_base}.md")
    with open(md_path, "w", encoding="utf-8") as file:
        file.write(md_content)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ PDF
    pdf_path = os.path.join(SAVE_DIR, f"{filename_base}.pdf")
    pdfkit.from_string(markdown(md_content), pdf_path)

    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img_tags = soup.find_all("img")
    for img in img_tags:
        img_url = img.get("src")
        if img_url and img_url.startswith("http"):
            img_name = os.path.basename(img_url).split("?")[0]
            img_path = os.path.join(IMG_DIR, img_name)
            img_data = requests.get(img_url).content
            with open(img_path, "wb") as img_file:
                img_file.write(img_data)

    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title} (HTML, MD, PDF)")

# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞
def main():
    print("üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π JavaRush...")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏
    article_links = get_article_links()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å–∏
    for i, url in enumerate(article_links, 1):
        print(f"[{i}/{len(article_links)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {url}")
        save_article(url)
        sleep(1)

    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ CSV
    csv_path = os.path.join(SAVE_DIR, "articles.csv")
    df = pd.read_sql_query("SELECT title, url, category, saved_at FROM articles", conn)
    df.to_csv(csv_path, index=False, encoding="utf-8")
    print(f"üìÑ CSV-—Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {csv_path}")

    print("‚úÖ –í—Å–µ —Å—Ç–∞—Ç—å–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

if __name__ == "__main__":
    main()
