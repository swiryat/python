import os
import requests
import sqlite3
import pdfkit
import markdown
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from time import sleep
from urllib.parse import urljoin

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–∏–Ω–∞ –∏ –ø–∞—Ä–æ–ª—è –∏–∑ .env
load_dotenv()
USERNAME = os.getenv("USERNAME")
PASSWORD = os.getenv("PASSWORD")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π
BASE_URL = "https://javarush.com"
LOGIN_URL = BASE_URL + "/user/login"
ARTICLES_URL = BASE_URL + "/all-articles"
SAVE_DIR = "JavaRush_Articles"
IMG_DIR = os.path.join(SAVE_DIR, "images")
DB_FILE = "articles.db"

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫–∏
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
conn = sqlite3.connect(DB_FILE)
cursor = conn.cursor()
cursor.execute("""
CREATE TABLE IF NOT EXISTS articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,
    url TEXT UNIQUE,
    category TEXT,
    saved_html TEXT,
    saved_markdown TEXT,
    saved_pdf TEXT,
    date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
""")
conn.commit()

# –°–µ—Å—Å–∏—è –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
session = requests.Session()

def get_csrf_token():
    """ –ü–æ–ª—É—á–∞–µ–º CSRF-—Ç–æ–∫–µ–Ω –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è) """
    response = session.get(LOGIN_URL)
    soup = BeautifulSoup(response.text, "html.parser")
    token = soup.find("input", {"name": "_csrf"})
    return token["value"] if token else None

def login():
    """ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ JavaRush """
    print("üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ...")

    csrf_token = get_csrf_token()
    login_payload = {
        "username": "zatura55@mail.ru",
        "password": "Masa34!masa35!"
    }
    
    if csrf_token:
        login_payload["_csrf"] = csrf_token

    response = session.post(LOGIN_URL, data=login_payload)

    if response.status_code == 200 and "logout" in response.text:
        print("‚úÖ –£—Å–ø–µ—à–Ω—ã–π –≤—Ö–æ–¥!")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏–Ω –∏ –ø–∞—Ä–æ–ª—å.")
        exit()

def get_articles():
    """ –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π """
    print("üîé –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π...")
    response = session.get(ARTICLES_URL)

    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ARTICLES_URL}")
        exit()

    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if "/groups/posts/" in href:
            full_url = urljoin(BASE_URL, href)
            articles.append(full_url)

    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π.")
    return articles

def save_article(article_url, index, total):
    """ –ü–∞—Ä—Å–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å—é –≤ HTML, Markdown –∏ PDF """
    print(f"[{index}/{total}] üì• –ó–∞–≥—Ä—É–∂–∞–µ–º: {article_url}")

    cursor.execute("SELECT saved_html FROM articles WHERE url = ?", (article_url,))
    if cursor.fetchone():
        print("‚è≠Ô∏è –°—Ç–∞—Ç—å—è —É–∂–µ —Å–∫–∞—á–∞–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return

    response = session.get(article_url)
    if response.status_code != 200:
        print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    
    title_tag = soup.find("h1")
    title = title_tag.text.strip() if title_tag else f"article_{index}"
    
    category_tag = soup.find("span", class_="category")
    category = category_tag.text.strip() if category_tag else "–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"

    safe_title = "".join(c if c.isalnum() or c in " _-" else "_" for c in title)

    # –ü—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    html_path = os.path.join(SAVE_DIR, safe_title + ".html")
    md_path = os.path.join(SAVE_DIR, safe_title + ".md")
    pdf_path = os.path.join(SAVE_DIR, safe_title + ".pdf")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML
    with open(html_path, "w", encoding="utf-8") as file:
        file.write(response.text)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown
    content_div = soup.find("div", class_="content")
    article_text = content_div.get_text("\n") if content_div else ""
    md_content = f"# {title}\n\n{article_text}"

    with open(md_path, "w", encoding="utf-8") as file:
        file.write(md_content)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PDF
    pdfkit.from_file(html_path, pdf_path)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    cursor.execute("INSERT INTO articles (title, url, category, saved_html, saved_markdown, saved_pdf) VALUES (?, ?, ?, ?, ?, ?)",
                   (title, article_url, category, html_path, md_path, pdf_path))
    conn.commit()

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {html_path}, {md_path}, {pdf_path}")

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    for img_tag in soup.find_all("img", src=True):
        img_url = urljoin(BASE_URL, img_tag["src"])
        img_name = os.path.basename(img_url)
        img_path = os.path.join(IMG_DIR, img_name)

        img_response = session.get(img_url, stream=True)
        if img_response.status_code == 200:
            with open(img_path, "wb") as img_file:
                for chunk in img_response.iter_content(1024):
                    img_file.write(chunk)
            print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {img_path}")

    sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫

def main():
    login()
    articles = get_articles()
    for i, article_url in enumerate(articles, 1):
        save_article(article_url, i, len(articles))
    print("üéâ –í—Å–µ —Å—Ç–∞—Ç—å–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î
    conn.close()

if __name__ == "__main__":
    main()
