import os
import re
import sqlite3
import requests
import pdfkit
import pandas as pd
from bs4 import BeautifulSoup
from time import sleep
from markdown import markdown
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
session = requests.Session()

# –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à–∏ cookies, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –ø–æ–ª—É—á–∏–ª–∏ –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
session.cookies.set("jr-sidebar-mode", "FULL")
session.cookies.set("javarush.user.id", "3425706")
session.cookies.set("JSESSIONID", "48a37049-9d45-46f4-b708-1cac06bd96e8")
session.cookies.set("intercom-session-mqlef7yz", "")
session.cookies.set("intercom-device-id-mqlef7yz", "e30412d1-7c2f-4c73-86be-930bfa384cb5")
session.cookies.set("__stripe_mid", "d92df48d-b7c8-45f7-a6a6-e7da35f218ce1dfa20")
session.cookies.set("CookieConsent", "{stamp:'-1',necessary:true,preferences:true,statistics:true,marketing:true,method:'implied',ver:1,utc:1741007185215,region:'GB'}")
session.cookies.set("jr-featured-content-14", "closed")
session.cookies.set("javarush.daynight", "light")
session.cookies.set("_gcl_au", "1.1.84417750.1741713789")
session.cookies.set("_ga", "GA1.1.346389540.1741713790")
session.cookies.set("_ga_G0F5YPM3KY", "deleted")
session.cookies.set("jr-sidebar-group-university-collapsed", "0")
session.cookies.set("jr-install-mark", "NOT_NOW")
session.cookies.set("jr-last-route", "%2Fall-articles")
session.cookies.set("_ga_G0F5YPM3KY", "GS1.1.1741823636.7.1.1741823935.60.1.1267399023")

# –ë–∞–∑–æ–≤—ã–π URL
BASE_URL = "https://javarush.com"
ARTICLES_URL = BASE_URL + "/all-articles"

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
SAVE_DIR = "JavaRush_Articles"
IMG_DIR = os.path.join(SAVE_DIR, "images")
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ SQLite
DB_PATH = os.path.join(SAVE_DIR, "articles.db")
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
cursor.execute("""
CREATE TABLE IF NOT EXISTS articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,
    url TEXT UNIQUE,
    category TEXT,
    saved_at TEXT
)
""")
conn.commit()

# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
def get_article_links():
    links = set()
    page = 1
    while True:
        url = f"{ARTICLES_URL}?page={page}"
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}...")

        response = session.get(url)
        if response.status_code != 200:
            print("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏, –∑–∞–≤–µ—Ä—à–∞–µ–º.")
            break

        soup = BeautifulSoup(response.text, "html.parser")
        new_links = {
            BASE_URL + a["href"]
            for a in soup.find_all("a", href=True)
            if "/groups/posts/" in a["href"]
        }

        if not new_links:
            break

        links.update(new_links)
        page += 1
        sleep(0.1)  # –£–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

    print(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Ç–∞—Ç–µ–π.")
    return links

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–æ—Ç–æ–∫–∞—Ö)
def save_article(article_url):
    response = session.get(article_url)  
    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ {response.status_code}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º {article_url}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_tag = soup.find("h1")
    title = title_tag.text.strip() if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    filename_base = re.sub(r"[^\w\-_]", "_", title)

    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
    category = "–†–∞–∑–Ω–æ–µ"
    for cat in ["Java", "Python", "–ê–ª–≥–æ—Ä–∏—Ç–º—ã"]:
        if cat.lower() in title.lower():
            category = cat
            break

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ë–î
    cursor.execute("SELECT id FROM articles WHERE url=?", (article_url,))
    if cursor.fetchone():
        print(f"–°—Ç–∞—Ç—å—è '{title}' —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return None

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    cursor.execute("INSERT INTO articles (title, url, category, saved_at) VALUES (?, ?, ?, datetime('now'))",
                   (title, article_url, category))
    conn.commit()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML
    html_path = os.path.join(SAVE_DIR, f"{filename_base}.html")
    with open(html_path, "w", encoding="utf-8") as file:
        file.write(response.text)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Markdown
    article_text = soup.find("article").text if soup.find("article") else "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞"
    md_content = f"# {title}\n\n{article_url}\n\n{article_text}"
    md_path = os.path.join(SAVE_DIR, f"{filename_base}.md")
    with open(md_path, "w", encoding="utf-8") as file:
        file.write(md_content)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ PDF
    pdf_path = os.path.join(SAVE_DIR, f"{filename_base}.pdf")
    pdfkit.from_string(markdown(md_content), pdf_path)

    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    for img in soup.find_all("img"):
        img_url = img.get("src")
        if img_url and img_url.startswith("http"):
            img_name = os.path.basename(img_url).split("?")[0]
            img_path = os.path.join(IMG_DIR, img_name)
            img_data = requests.get(img_url).content
            with open(img_path, "wb") as img_file:
                img_file.write(img_data)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title} (HTML, MD, PDF)")

# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ (—Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é)
def main():
    print("üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π JavaRush...")

    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏
    article_links = get_article_links()

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
    MAX_THREADS = 200

    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_url = {executor.submit(save_article, url): url for url in article_links}

        for future in as_completed(future_to_url):
            try:
                future.result()  # –û–∂–∏–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")

    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ CSV
    csv_path = os.path.join(SAVE_DIR, "articles.csv")
    df = pd.read_sql_query("SELECT title, url, category, saved_at FROM articles", conn)
    df.to_csv(csv_path, index=False, encoding="utf-8")
    print(f"üìÑ CSV-—Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {csv_path}")

    print("‚úÖ –í—Å–µ —Å—Ç–∞—Ç—å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

if __name__ == "__main__":
    main()
