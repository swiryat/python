{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Подключаем Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "\n",
        "# Создаём папку, если её нет\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IkE0u7HVptU",
        "outputId": "25df4517-6c5b-4245-8e09-09caf164f0a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F4olS3JtlwAD",
        "outputId": "af9b1ea4-4494-4ba3-8ccd-6e8d5ff588b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7CB5EptFrKuY",
        "outputId": "82311a7f-e0da-4ef2-de1e-7547d3bcc7d1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import sqlite3\n",
        "import pdfplumber  # ✅ Заменили PyPDF2 на pdfplumber\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "vXYw8atubyVv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🗂 Пути для хранения файлов\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "PDF_FOLDER = \"/content/drive/MyDrive/pdf_micro2\"\n",
        "TOKENIZER_PATH = os.path.join(GDRIVE_PATH, \"tokenizer.pkl\")\n",
        "MODEL_PATH = '/content/drive/MyDrive/colab_pdf_model/model.keras'  # Убедитесь, что файл действительно находится здесь\n",
        "model = load_model(MODEL_PATH)\n",
        "DB_PATH = os.path.join(GDRIVE_PATH, \"dataset.db\")"
      ],
      "metadata": {
        "id": "GIJDT3M0YgS8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Создаём папки, если их нет\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)"
      ],
      "metadata": {
        "id": "RUNeWPScYkM8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔄 Функция загрузки или создания токенизатора\n",
        "def load_or_create_tokenizer():\n",
        "    if os.path.exists(TOKENIZER_PATH):\n",
        "        print(\"📥 Загружаем существующий токенизатор...\")\n",
        "        with open(TOKENIZER_PATH, \"rb\") as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        print(\"🆕 Создаём новый токенизатор...\")\n",
        "        tokenizer = Tokenizer()\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "BlExAphtYlg-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔄 Функция сохранения токенизатора\n",
        "def save_tokenizer(tokenizer):\n",
        "    with open(TOKENIZER_PATH, \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    print(\"💾 Токенизатор сохранён.\")"
      ],
      "metadata": {
        "id": "gQf4chyxYqTF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔄 Функция загрузки или создания модели\n",
        "def load_or_create_model(vocab_size):\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"📥 Загружаем обученную модель...\")\n",
        "        model = load_model(MODEL_PATH)\n",
        "    else:\n",
        "        print(\"🆕 Создаём новую модель...\")\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, 128, input_length=100),\n",
        "            LSTM(128, return_sequences=True),\n",
        "            LSTM(64),\n",
        "            Dense(64, activation=\"relu\"),\n",
        "            Dense(vocab_size, activation=\"softmax\")\n",
        "        ])\n",
        "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "dfH0uedGYtPs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📄 Функция чтения текста из PDF с pdfplumber\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Ошибка при обработке {pdf_path}: {e}\")\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "VOAJ_qzbYw4M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📂 Собираем текст из PDF\n",
        "all_texts = []\n",
        "pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
        "if not pdf_files:\n",
        "    print(\"❌ В папке нет PDF-файлов. Загрузите файлы в Google Drive → pdf_micro.\")\n",
        "\n",
        "for filename in pdf_files:\n",
        "    pdf_path = os.path.join(PDF_FOLDER, filename)\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    if text:\n",
        "        all_texts.append(text)\n"
      ],
      "metadata": {
        "id": "7A8Nqk0kY0U8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Загружаем или создаём токенизатор\n",
        "tokenizer = load_or_create_tokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA9fPn-AY658",
        "outputId": "a3cc1d32-4534-4d9c-e7d7-f5f544110218"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Загружаем существующий токенизатор...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Обновляем токенизатор\n",
        "if all_texts:\n",
        "    tokenizer.fit_on_texts(all_texts)\n",
        "    save_tokenizer(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDATNWK3Y8Cd",
        "outputId": "611a51d1-b75d-434a-c5e8-f9b388ad8570"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Токенизатор сохранён.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Преобразуем текст в последовательности\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding=\"post\")"
      ],
      "metadata": {
        "id": "GX7aJ9sLY_Ws"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🎯 Загружаем или создаём модель\n",
        "vocab_size = len(word_index) + 1\n",
        "model = load_or_create_model(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlRExvmdZCxM",
        "outputId": "f0e6d618-4d1a-4885-f7a2-6477adbf5fd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Загружаем обученную модель...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 💾 Функция сохранения модели\n",
        "def save_model():\n",
        "    model.save(MODEL_PATH)\n",
        "    print(\"💾 Модель сохранена в Google Drive.\")"
      ],
      "metadata": {
        "id": "Jodtzi5vZFhE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛠 Сохраняем текст в базу данных SQLite\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"CREATE TABLE IF NOT EXISTS documents (id INTEGER PRIMARY KEY, text TEXT)\")\n",
        "for text in all_texts:\n",
        "    cursor.execute(\"INSERT INTO documents (text) VALUES (?)\", (text,))\n",
        "conn.commit()\n",
        "conn.close()\n",
        "print(\"📂 Данные сохранены в dataset.db\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v5eY55YZIMt",
        "outputId": "8b74c65b-de04-4a9f-a5f6-759049f103d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Данные сохранены в dataset.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим размерность обучающих данных\n",
        "print(\"Размерность обучающих данных:\", X_train.shape)  # Должно быть (n_samples, 10)\n",
        "\n",
        "# Проверим размерность данных, которые нужно преобразовать\n",
        "print(\"Размерность текущих данных:\", X.shape)  # Должно быть (n_samples, 10)\n",
        "\n",
        "# Проверим размерность данных X\n",
        "print(\"Размерность данных X:\", X.shape)\n",
        "\n",
        "# Проверим размерность данных, на которых обучался скейлер\n",
        "print(\"Размерность данных, на которых обучался скейлер:\", scaler.n_features_in_)\n",
        "\n",
        "# Если размерности не совпадают, можно привести их к одинаковому виду:\n",
        "# Например, если в X отсутствуют некоторые признаки, можно добавить их с нулями или соответствующими значениями.\n",
        "if X.shape[1] != scaler.n_features_in_:\n",
        "    print(\"Размерности данных и скейлера не совпадают. Приводим данные к нужному виду.\")\n",
        "    # Возможно, добавим недостающие признаки с нулями\n",
        "    missing_columns = scaler.n_features_in_ - X.shape[1]\n",
        "    X = np.hstack([X, np.zeros((X.shape[0], missing_columns))])  # Добавляем нули, если признаки отсутствуют\n",
        "\n",
        "# Печатаем все колонки в датасете для проверки\n",
        "print(\"Колонки в датасете:\", data.columns.tolist())\n",
        "\n",
        "# Применение токенизатора: Проверим, что tokenizer обучен\n",
        "if hasattr(tokenizer, 'word_index'):\n",
        "    print(\"Словарь индексов токенизатора:\", tokenizer.word_index)\n",
        "else:\n",
        "    print(\"Ошибка: Токенизатор не обучен\")\n",
        "\n",
        "# Пример текста для предсказания\n",
        "seed_text = \"java\"\n",
        "\n",
        "# Преобразуем seed_text в последовательность индексов\n",
        "sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "print(\"Индексированная последовательность:\", sequence)  # Выводим последовательность после преобразования"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UYDygyE6_90",
        "outputId": "0f592f7f-a27e-4992-d72b-de751f6e08c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность обучающих данных: (8, 8)\n",
            "Размерность текущих данных: (10, 8)\n",
            "Размерность данных X: (10, 8)\n",
            "Размерность данных, на которых обучался скейлер: 8\n",
            "Колонки в датасете: ['id', 'text']\n",
            "Словарь индексов токенизатора: {'<OOV>': 1, 'the': 2, 'quick': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'hello': 10, 'world': 11}\n",
            "Индексированная последовательность: [[1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 🗂 Пути\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "MODEL_PATH = os.path.join(GDRIVE_PATH, \"model.keras\")\n",
        "SCALER_PATH = os.path.join(GDRIVE_PATH, \"new_scaler.pkl\")\n",
        "DB_PATH = os.path.join(GDRIVE_PATH, \"dataset.db\")\n",
        "\n",
        "# Создаём директорию, если её нет\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# 🔹 Загружаем данные из базы SQLite\n",
        "if not os.path.exists(DB_PATH):\n",
        "    raise FileNotFoundError(f\"❌ Файл базы данных {DB_PATH} не найден! Проверь путь.\")\n",
        "\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "\n",
        "# Проверяем список таблиц\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = pd.read_sql(query, conn)\n",
        "\n",
        "print(\"📌 Таблицы в базе данных:\")\n",
        "print(tables)\n",
        "\n",
        "# Выбираем данные из таблицы documents (поменяй на нужное имя)\n",
        "TABLE_NAME = \"documents\"  # Укажи правильное имя таблицы\n",
        "if TABLE_NAME not in tables[\"name\"].values:\n",
        "    raise ValueError(f\"❌ Таблица '{TABLE_NAME}' не найдена в базе данных!\")\n",
        "\n",
        "# Загружаем данные\n",
        "data = pd.read_sql(f\"SELECT * FROM {TABLE_NAME};\", conn)\n",
        "\n",
        "# Закрываем соединение\n",
        "conn.close()\n",
        "\n",
        "print(f\"✅ Данные загружены из таблицы {TABLE_NAME}\")\n",
        "print(data.head())  # Выводим первые строки\n",
        "\n",
        "# 🔹 Разделяем признаки и целевую переменную\n",
        "target_column = \"id\"  # ❗ Замени на правильное имя целевой переменной\n",
        "if target_column not in data.columns:\n",
        "    raise ValueError(f\"❌ В датасете нет колонки {target_column}! Проверь названия столбцов.\")\n",
        "\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# 🔹 Преобразуем текстовые данные в числовые (если есть такие)\n",
        "text_columns = X.select_dtypes(include=['object']).columns\n",
        "if len(text_columns) > 0:\n",
        "    print(f\"📌 Преобразуем текстовые столбцы: {text_columns}\")\n",
        "    # Используем OneHotEncoder для преобразования текстовых данных в числовые\n",
        "    transformer = ColumnTransformer([\n",
        "        ('text', OneHotEncoder(), text_columns),\n",
        "    ], remainder='passthrough')  # Оставляем остальные столбцы без изменений\n",
        "\n",
        "    X = transformer.fit_transform(X)\n",
        "    # Сохраняем трансформатор для дальнейшего использования\n",
        "    joblib.dump(transformer, SCALER_PATH.replace(\"scaler\", \"transformer\"))\n",
        "    print(f\"✅ Трансформатор сохранён в {SCALER_PATH.replace('scaler', 'transformer')}\")\n",
        "else:\n",
        "    print(\"📌 Текстовых данных нет, пропускаем преобразование.\")\n",
        "\n",
        "# 🔹 Нормализация данных\n",
        "if os.path.exists(SCALER_PATH):\n",
        "    scaler = joblib.load(SCALER_PATH)\n",
        "    print(\"📌 Загружен существующий scaler\")\n",
        "else:\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X)  # Применяем масштабирование ко всему набору данных до разделения\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "    print(f\"✅ Scaler сохранен в {SCALER_PATH}\")\n",
        "\n",
        "X = scaler.transform(X)  # Применяем нормализацию ко всем данным\n",
        "\n",
        "# 🔹 Разделение на обучающую и валидационную выборки\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"✅ Размеры выборок: X_train: {X_train.shape}, X_val: {X_val.shape}\")\n",
        "\n",
        "# Преобразуем данные с использованием трансформатора\n",
        "if os.path.exists(SCALER_PATH.replace(\"scaler\", \"transformer\")):\n",
        "    transformer = joblib.load(SCALER_PATH.replace(\"scaler\", \"transformer\"))\n",
        "    print(\"📌 Загружен существующий трансформатор\")\n",
        "else:\n",
        "    raise ValueError(\"❌ Не удалось найти трансформатор! Проверь путь.\")\n",
        "\n",
        "# Применяем трансформатор к данным\n",
        "X_train_transformed = transformer.transform(X_train)\n",
        "X_val_transformed = transformer.transform(X_val)\n",
        "\n",
        "# Преобразуем разреженные матрицы в плотные\n",
        "X_train_transformed = X_train_transformed.toarray()  # Плотная матрица\n",
        "X_val_transformed = X_val_transformed.toarray()  # Плотная матрица\n",
        "\n",
        "# Применяем нормализацию только после трансформации\n",
        "X_train_transformed = scaler.transform(X_train_transformed)\n",
        "X_val_transformed = scaler.transform(X_val_transformed)\n",
        "\n",
        "\n",
        "# 🔹 Определяем архитектуру модели\n",
        "def build_model(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        Dense(128, activation=tf.keras.activations.swish, input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation=tf.keras.activations.swish),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation=tf.keras.activations.swish),\n",
        "        Dense(1, activation=\"linear\")  # Для регрессии\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss=tf.keras.losses.Huber(delta=1.5),\n",
        "                  metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "# 🔹 Загружаем модель (если уже обучена)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model = load_model(MODEL_PATH)\n",
        "    print(f\"📌 Загружена существующая модель из {MODEL_PATH}\")\n",
        "else:\n",
        "    input_shape = (X_train.shape[1],)\n",
        "    model = build_model(input_shape)\n",
        "    print(\"✅ Создана новая модель\")\n",
        "\n",
        "# 🔹 Коллбэки\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0),\n",
        "    ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "]\n",
        "\n",
        "# 🔹 Запускаем обучение\n",
        "history = model.fit(X_train_transformed, y_train,\n",
        "                    validation_data=(X_val_transformed, y_val),\n",
        "                    epochs=70,\n",
        "                    batch_size=64,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# 🔹 Сохраняем модель\n",
        "model.save(MODEL_PATH)\n",
        "print(f\"✅ Модель сохранена в {MODEL_PATH}\")\n",
        "\n",
        "# 🔹 Построение графиков обучения\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_loss, label=\"Train Loss\", color=\"blue\")\n",
        "plt.plot(val_loss, label=\"Validation Loss\", color=\"orange\")\n",
        "plt.plot(train_mae, label=\"Train MAE\", color=\"green\")\n",
        "plt.plot(val_mae, label=\"Validation MAE\", color=\"red\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss / MAE\")\n",
        "plt.title(\"Model Training History\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "collapsed": true,
        "id": "UvNr8aJmZLVk",
        "outputId": "2fb3d2d9-8028-4fad-a334-e0dda54b0fae"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Таблицы в базе данных:\n",
            "        name\n",
            "0  documents\n",
            "✅ Данные загружены из таблицы documents\n",
            "   id                                               text\n",
            "0   1  Алексей Васильев\\nJAVA\\nД ЛЯ ВСЕХ\\nКНИГА О ПОП...\n",
            "1   2  Алексей Васильев\\nJAVA\\nД ЛЯ ВСЕХ\\nКНИГА О ПОП...\n",
            "2   3  Алексей Васильев\\nJAVA\\nД ЛЯ ВСЕХ\\nКНИГА О ПОП...\n",
            "3   4  (cid:1)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid...\n",
            "4   5  Дэвид Копец\\nКлассические задачи\\nComputer Sci...\n",
            "📌 Преобразуем текстовые столбцы: Index(['text'], dtype='object')\n",
            "✅ Трансформатор сохранён в /content/drive/MyDrive/colab_pdf_model/new_transformer.pkl\n",
            "📌 Загружен существующий scaler\n",
            "✅ Размеры выборок: X_train: (8, 8), X_val: (2, 8)\n",
            "📌 Загружен существующий трансформатор\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 8 features, but ColumnTransformer is expecting 1 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-a3ef53ae9d1c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# Применяем трансформатор к данным\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mX_train_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0mX_val_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0;31m# ndarray was used for fitting or transforming, thus we only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;31m# check that n_features_in_ is consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m             \u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2830\u001b[0m             \u001b[0;34mf\"X has {n_features} features, but {estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;34mf\"is expecting {estimator.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 8 features, but ColumnTransformer is expecting 1 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Пути для загрузки модели\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "MODEL_PATH = os.path.join(GDRIVE_PATH, \"model.keras\")  # Указываем правильный путь к модели .keras\n",
        "TOKENIZER_PATH = os.path.join(GDRIVE_PATH, \"tokenizer.pkl\")\n",
        "\n",
        "# Загружаем токенизатор\n",
        "with open(TOKENIZER_PATH, \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Загружаем модель\n",
        "model = load_model(MODEL_PATH)\n",
        "print(\"Модель и токенизатор загружены!\")\n",
        "\n",
        "# Функция для сохранения модели\n",
        "def save_model(model, path='model.keras'):\n",
        "    \"\"\"Сохраняет модель по заданному пути.\"\"\"\n",
        "    model.save(path)\n",
        "    print(f\"Модель сохранена в {path}\")\n",
        "\n",
        "# Основная функция для генерации текста\n",
        "def generate_text(seed_text, next_words=50, save_after_generation=True):\n",
        "    \"\"\"Генерирует текст на основе начальной строки (seed_text) и сохраняет модель после генерации.\"\"\"\n",
        "    max_sequence_length = 10  # Замените на нужную длину, если она отличается\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Преобразуем текст в последовательность\n",
        "        sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "\n",
        "        # Паддируем последовательность до нужной длины\n",
        "        padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding=\"pre\")\n",
        "\n",
        "        # Предсказываем следующее слово\n",
        "        predicted_index = np.argmax(model.predict(padded_sequence), axis=-1)[0]\n",
        "\n",
        "        # Находим слово по индексу\n",
        "        word = None\n",
        "        for w, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                word = w\n",
        "                break\n",
        "\n",
        "        # Добавляем слово к предложению\n",
        "        if word:\n",
        "            seed_text += \" \" + word\n",
        "        else:\n",
        "            break  # Если слово не найдено, останавливаемся\n",
        "\n",
        "    if save_after_generation:\n",
        "        # Сохраняем модель после генерации текста\n",
        "        save_model(model)\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Пример использования\n",
        "generated_text = generate_text(\"println\")\n",
        "print(\"Сгенерированный текст:\", generated_text)\n",
        "\n",
        "# Сохранение модели в формате .keras (если это нужно по завершению работы)\n",
        "save_model(model, 'model.keras')\n",
        "\n",
        "# Если нужно скачать модель из Google Colab\n",
        "#from google.colab import files\n",
        "#files.download('model.keras')  # Скачиваем модель на ваш локальный компьютер\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8EaU89FbQir",
        "outputId": "25c848ba-2085-4a86-8297-767109594129"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель и токенизатор загружены!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "Модель сохранена в model.keras\n",
            "Сгенерированный текст: println\n",
            "Модель сохранена в model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачиваем модель\n",
        "files.download(MODEL_PATH)\n",
        "\n",
        "# Скачиваем токенизатор\n",
        "files.download(TOKENIZER_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FUm68y5hbb2z",
        "outputId": "a35d9147-db2a-43c2-e7c5-d78926cfc089"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b1ab2f81-d4b8-47e0-9669-c21a9b36dd36\", \"model.keras\", 144447)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f549c17-6613-4bac-8c62-8749a9f34cc3\", \"tokenizer.pkl\", 532131)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Пример текста\n",
        "seed_text = \"The quick brown fox jumps\"\n",
        "\n",
        "# Инициализация токенизатора с учётом OOV токена\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "# Тренируем токенизатор на ваших данных\n",
        "text_data = [\"The quick brown fox jumps\", \"Over the lazy dog\", \"Hello world\"]  # Пример данных для обучения\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "\n",
        "# Преобразуем текст в последовательность индексов\n",
        "sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "print(\"Индексированная последовательность:\", sequence)\n",
        "\n",
        "# Паддирование последовательности до фиксированной длины\n",
        "max_sequence_length = 10  # Максимальная длина последовательности\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='pre')\n",
        "print(\"Паддированная последовательность:\", padded_sequence)\n",
        "\n",
        "# Прогон через модель (предполагаем, что модель уже обучена)\n",
        "# Создание простой модели для примера\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_sequence_length),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "# Для теста прогоняем модель с паддированной последовательностью\n",
        "predicted_probs = model.predict(padded_sequence)  # Вероятности для всех слов\n",
        "predicted_index = np.argmax(predicted_probs, axis=-1)[0]  # Индекс предсказанного слова\n",
        "print(f\"Предсказанный индекс: {predicted_index}\")\n",
        "\n",
        "# Получаем слово из индекса\n",
        "word = None\n",
        "for w, index in tokenizer.word_index.items():\n",
        "    if index == predicted_index:\n",
        "        word = w\n",
        "        break\n",
        "\n",
        "print(f\"Предсказанное слово: {word}\")\n",
        "\n",
        "# Прогон через модель с температурой\n",
        "def predict_with_temperature(model, padded_sequence, temperature=0.7):\n",
        "    predictions = model.predict(padded_sequence)[0]\n",
        "    predictions = np.log(predictions + 1e-7) / temperature  # Логарифмируем и применяем температуру\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)  # Приводим к нормализованным вероятностям\n",
        "    return np.random.choice(len(predictions), p=predictions)\n",
        "\n",
        "# Применяем функцию с температурой\n",
        "predicted_index_temp = predict_with_temperature(model, padded_sequence, temperature=0.7)\n",
        "print(f\"Предсказанный индекс с температурой: {predicted_index_temp}\")\n",
        "\n",
        "# Проверяем все слова и их индексы в токенизаторе\n",
        "print(\"\\nВсе слова и их индексы в токенизаторе:\")\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    print(f\"{word}: {index}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Na-gQa_t8N",
        "outputId": "869b4854-481a-4bbb-d1cb-b6c086e47b56"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Индексированная последовательность: [[2, 3, 4, 5, 6]]\n",
            "Паддированная последовательность: [[0 0 0 0 0 2 3 4 5 6]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
            "Предсказанный индекс: 10\n",
            "Предсказанное слово: hello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Предсказанный индекс с температурой: 10\n",
            "\n",
            "Все слова и их индексы в токенизаторе:\n",
            "<OOV>: 1\n",
            "the: 2\n",
            "quick: 3\n",
            "brown: 4\n",
            "fox: 5\n",
            "jumps: 6\n",
            "over: 7\n",
            "lazy: 8\n",
            "dog: 9\n",
            "hello: 10\n",
            "world: 11\n"
          ]
        }
      ]
    }
  ]
}