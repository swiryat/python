{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# –ü–æ–¥–∫–ª—é—á–∞–µ–º Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IkE0u7HVptU",
        "outputId": "25df4517-6c5b-4245-8e09-09caf164f0a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F4olS3JtlwAD",
        "outputId": "af9b1ea4-4494-4ba3-8ccd-6e8d5ff588b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7CB5EptFrKuY",
        "outputId": "82311a7f-e0da-4ef2-de1e-7547d3bcc7d1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import sqlite3\n",
        "import pdfplumber  # ‚úÖ –ó–∞–º–µ–Ω–∏–ª–∏ PyPDF2 –Ω–∞ pdfplumber\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "vXYw8atubyVv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üóÇ –ü—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "PDF_FOLDER = \"/content/drive/MyDrive/pdf_micro2\"\n",
        "TOKENIZER_PATH = os.path.join(GDRIVE_PATH, \"tokenizer.pkl\")\n",
        "MODEL_PATH = '/content/drive/MyDrive/colab_pdf_model/model.keras'  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∑–¥–µ—Å—å\n",
        "model = load_model(MODEL_PATH)\n",
        "DB_PATH = os.path.join(GDRIVE_PATH, \"dataset.db\")"
      ],
      "metadata": {
        "id": "GIJDT3M0YgS8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)"
      ],
      "metadata": {
        "id": "RUNeWPScYkM8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÑ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "def load_or_create_tokenizer():\n",
        "    if os.path.exists(TOKENIZER_PATH):\n",
        "        print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
        "        with open(TOKENIZER_PATH, \"rb\") as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        print(\"üÜï –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
        "        tokenizer = Tokenizer()\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "BlExAphtYlg-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÑ –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "def save_tokenizer(tokenizer):\n",
        "    with open(TOKENIZER_PATH, \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    print(\"üíæ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω.\")"
      ],
      "metadata": {
        "id": "gQf4chyxYqTF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÑ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
        "def load_or_create_model(vocab_size):\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...\")\n",
        "        model = load_model(MODEL_PATH)\n",
        "    else:\n",
        "        print(\"üÜï –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...\")\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, 128, input_length=100),\n",
        "            LSTM(128, return_sequences=True),\n",
        "            LSTM(64),\n",
        "            Dense(64, activation=\"relu\"),\n",
        "            Dense(vocab_size, activation=\"softmax\")\n",
        "        ])\n",
        "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "dfH0uedGYtPs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÑ –§—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Å pdfplumber\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_path}: {e}\")\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "VOAJ_qzbYw4M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ PDF\n",
        "all_texts = []\n",
        "pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
        "if not pdf_files:\n",
        "    print(\"‚ùå –í –ø–∞–ø–∫–µ –Ω–µ—Ç PDF-—Ñ–∞–π–ª–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ Google Drive ‚Üí pdf_micro.\")\n",
        "\n",
        "for filename in pdf_files:\n",
        "    pdf_path = os.path.join(PDF_FOLDER, filename)\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    if text:\n",
        "        all_texts.append(text)\n"
      ],
      "metadata": {
        "id": "7A8Nqk0kY0U8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "tokenizer = load_or_create_tokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA9fPn-AY658",
        "outputId": "a3cc1d32-4534-4d9c-e7d7-f5f544110218"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "if all_texts:\n",
        "    tokenizer.fit_on_texts(all_texts)\n",
        "    save_tokenizer(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDATNWK3Y8Cd",
        "outputId": "611a51d1-b75d-434a-c5e8-f9b388ad8570"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding=\"post\")"
      ],
      "metadata": {
        "id": "GX7aJ9sLY_Ws"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å\n",
        "vocab_size = len(word_index) + 1\n",
        "model = load_or_create_model(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlRExvmdZCxM",
        "outputId": "f0e6d618-4d1a-4885-f7a2-6477adbf5fd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üíæ –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
        "def save_model():\n",
        "    model.save(MODEL_PATH)\n",
        "    print(\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ Google Drive.\")"
      ],
      "metadata": {
        "id": "Jodtzi5vZFhE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ† –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö SQLite\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"CREATE TABLE IF NOT EXISTS documents (id INTEGER PRIMARY KEY, text TEXT)\")\n",
        "for text in all_texts:\n",
        "    cursor.execute(\"INSERT INTO documents (text) VALUES (?)\", (text,))\n",
        "conn.commit()\n",
        "conn.close()\n",
        "print(\"üìÇ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ dataset.db\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v5eY55YZIMt",
        "outputId": "8b74c65b-de04-4a9f-a5f6-759049f103d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ dataset.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:\", X_train.shape)  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å (n_samples, 10)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å\n",
        "print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:\", X.shape)  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å (n_samples, 10)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö X\n",
        "print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö X:\", X.shape)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—É—á–∞–ª—Å—è —Å–∫–µ–π–ª–µ—Ä\n",
        "print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—É—á–∞–ª—Å—è —Å–∫–µ–π–ª–µ—Ä:\", scaler.n_features_in_)\n",
        "\n",
        "# –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –º–æ–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∏—Ö –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º—É –≤–∏–¥—É:\n",
        "# –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤ X –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏—Ö —Å –Ω—É–ª—è–º–∏ –∏–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.\n",
        "if X.shape[1] != scaler.n_features_in_:\n",
        "    print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–∫–µ–π–ª–µ—Ä–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç. –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –∫ –Ω—É–∂–Ω–æ–º—É –≤–∏–¥—É.\")\n",
        "    # –í–æ–∑–º–æ–∂–Ω–æ, –¥–æ–±–∞–≤–∏–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª—è–º–∏\n",
        "    missing_columns = scaler.n_features_in_ - X.shape[1]\n",
        "    X = np.hstack([X, np.zeros((X.shape[0], missing_columns))])  # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–∏, –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç\n",
        "\n",
        "# –ü–µ—á–∞—Ç–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
        "print(\"–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\", data.columns.tolist())\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ tokenizer –æ–±—É—á–µ–Ω\n",
        "if hasattr(tokenizer, 'word_index'):\n",
        "    print(\"–°–ª–æ–≤–∞—Ä—å –∏–Ω–¥–µ–∫—Å–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:\", tokenizer.word_index)\n",
        "else:\n",
        "    print(\"–û—à–∏–±–∫–∞: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "seed_text = \"java\"\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º seed_text –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "print(\"–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\", sequence)  # –í—ã–≤–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UYDygyE6_90",
        "outputId": "0f592f7f-a27e-4992-d72b-de751f6e08c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: (8, 8)\n",
            "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: (10, 8)\n",
            "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö X: (10, 8)\n",
            "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—É—á–∞–ª—Å—è —Å–∫–µ–π–ª–µ—Ä: 8\n",
            "–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: ['id', 'text']\n",
            "–°–ª–æ–≤–∞—Ä—å –∏–Ω–¥–µ–∫—Å–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {'<OOV>': 1, 'the': 2, 'quick': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'hello': 10, 'world': 11}\n",
            "–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: [[1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# üóÇ –ü—É—Ç–∏\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "MODEL_PATH = os.path.join(GDRIVE_PATH, \"model.keras\")\n",
        "SCALER_PATH = os.path.join(GDRIVE_PATH, \"new_scaler.pkl\")\n",
        "DB_PATH = os.path.join(GDRIVE_PATH, \"dataset.db\")\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã SQLite\n",
        "if not os.path.exists(DB_PATH):\n",
        "    raise FileNotFoundError(f\"‚ùå –§–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö {DB_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å –ø—É—Ç—å.\")\n",
        "\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = pd.read_sql(query, conn)\n",
        "\n",
        "print(\"üìå –¢–∞–±–ª–∏—Ü—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(tables)\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã documents (–ø–æ–º–µ–Ω—è–π –Ω–∞ –Ω—É–∂–Ω–æ–µ –∏–º—è)\n",
        "TABLE_NAME = \"documents\"  # –£–∫–∞–∂–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è —Ç–∞–±–ª–∏—Ü—ã\n",
        "if TABLE_NAME not in tables[\"name\"].values:\n",
        "    raise ValueError(f\"‚ùå –¢–∞–±–ª–∏—Ü–∞ '{TABLE_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "data = pd.read_sql(f\"SELECT * FROM {TABLE_NAME};\", conn)\n",
        "\n",
        "# –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
        "conn.close()\n",
        "\n",
        "print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ç–∞–±–ª–∏—Ü—ã {TABLE_NAME}\")\n",
        "print(data.head())  # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "\n",
        "# üîπ –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "target_column = \"id\"  # ‚ùó –ó–∞–º–µ–Ω–∏ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "if target_column not in data.columns:\n",
        "    raise ValueError(f\"‚ùå –í –¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ {target_column}! –ü—Ä–æ–≤–µ—Ä—å –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤.\")\n",
        "\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# üîπ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —á–∏—Å–ª–æ–≤—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–∫–∏–µ)\n",
        "text_columns = X.select_dtypes(include=['object']).columns\n",
        "if len(text_columns) > 0:\n",
        "    print(f\"üìå –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {text_columns}\")\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º OneHotEncoder –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —á–∏—Å–ª–æ–≤—ã–µ\n",
        "    transformer = ColumnTransformer([\n",
        "        ('text', OneHotEncoder(), text_columns),\n",
        "    ], remainder='passthrough')  # –û—Å—Ç–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
        "\n",
        "    X = transformer.fit_transform(X)\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "    joblib.dump(transformer, SCALER_PATH.replace(\"scaler\", \"transformer\"))\n",
        "    print(f\"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {SCALER_PATH.replace('scaler', 'transformer')}\")\n",
        "else:\n",
        "    print(\"üìå –¢–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.\")\n",
        "\n",
        "# üîπ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "if os.path.exists(SCALER_PATH):\n",
        "    scaler = joblib.load(SCALER_PATH)\n",
        "    print(\"üìå –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scaler\")\n",
        "else:\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X)  # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ –≤—Å–µ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "    print(f\"‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {SCALER_PATH}\")\n",
        "\n",
        "X = scaler.transform(X)  # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º\n",
        "\n",
        "# üîπ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"‚úÖ –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: X_train: {X_train.shape}, X_val: {X_val.shape}\")\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∞\n",
        "if os.path.exists(SCALER_PATH.replace(\"scaler\", \"transformer\")):\n",
        "    transformer = joblib.load(SCALER_PATH.replace(\"scaler\", \"transformer\"))\n",
        "    print(\"üìå –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä! –ü—Ä–æ–≤–µ—Ä—å –ø—É—Ç—å.\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä –∫ –¥–∞–Ω–Ω—ã–º\n",
        "X_train_transformed = transformer.transform(X_train)\n",
        "X_val_transformed = transformer.transform(X_val)\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –≤ –ø–ª–æ—Ç–Ω—ã–µ\n",
        "X_train_transformed = X_train_transformed.toarray()  # –ü–ª–æ—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞\n",
        "X_val_transformed = X_val_transformed.toarray()  # –ü–ª–æ—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "X_train_transformed = scaler.transform(X_train_transformed)\n",
        "X_val_transformed = scaler.transform(X_val_transformed)\n",
        "\n",
        "\n",
        "# üîπ –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏\n",
        "def build_model(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        Dense(128, activation=tf.keras.activations.swish, input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation=tf.keras.activations.swish),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation=tf.keras.activations.swish),\n",
        "        Dense(1, activation=\"linear\")  # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss=tf.keras.losses.Huber(delta=1.5),\n",
        "                  metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —É–∂–µ –æ–±—É—á–µ–Ω–∞)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model = load_model(MODEL_PATH)\n",
        "    print(f\"üìå –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –º–æ–¥–µ–ª—å –∏–∑ {MODEL_PATH}\")\n",
        "else:\n",
        "    input_shape = (X_train.shape[1],)\n",
        "    model = build_model(input_shape)\n",
        "    print(\"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å\")\n",
        "\n",
        "# üîπ –ö–æ–ª–ª–±—ç–∫–∏\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0),\n",
        "    ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "]\n",
        "\n",
        "# üîπ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
        "history = model.fit(X_train_transformed, y_train,\n",
        "                    validation_data=(X_val_transformed, y_val),\n",
        "                    epochs=70,\n",
        "                    batch_size=64,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# üîπ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
        "model.save(MODEL_PATH)\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}\")\n",
        "\n",
        "# üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_loss, label=\"Train Loss\", color=\"blue\")\n",
        "plt.plot(val_loss, label=\"Validation Loss\", color=\"orange\")\n",
        "plt.plot(train_mae, label=\"Train MAE\", color=\"green\")\n",
        "plt.plot(val_mae, label=\"Validation MAE\", color=\"red\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss / MAE\")\n",
        "plt.title(\"Model Training History\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "collapsed": true,
        "id": "UvNr8aJmZLVk",
        "outputId": "2fb3d2d9-8028-4fad-a334-e0dda54b0fae"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå –¢–∞–±–ª–∏—Ü—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö:\n",
            "        name\n",
            "0  documents\n",
            "‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ç–∞–±–ª–∏—Ü—ã documents\n",
            "   id                                               text\n",
            "0   1  –ê–ª–µ–∫—Å–µ–π –í–∞—Å–∏–ª—å–µ–≤\\nJAVA\\n–î –õ–Ø –í–°–ï–•\\n–ö–ù–ò–ì–ê –û –ü–û–ü...\n",
            "1   2  –ê–ª–µ–∫—Å–µ–π –í–∞—Å–∏–ª—å–µ–≤\\nJAVA\\n–î –õ–Ø –í–°–ï–•\\n–ö–ù–ò–ì–ê –û –ü–û–ü...\n",
            "2   3  –ê–ª–µ–∫—Å–µ–π –í–∞—Å–∏–ª—å–µ–≤\\nJAVA\\n–î –õ–Ø –í–°–ï–•\\n–ö–ù–ò–ì–ê –û –ü–û–ü...\n",
            "3   4  (cid:1)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid...\n",
            "4   5  –î—ç–≤–∏–¥ –ö–æ–ø–µ—Ü\\n–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏\\nComputer Sci...\n",
            "üìå –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: Index(['text'], dtype='object')\n",
            "‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ /content/drive/MyDrive/colab_pdf_model/new_transformer.pkl\n",
            "üìå –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scaler\n",
            "‚úÖ –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: X_train: (8, 8), X_val: (2, 8)\n",
            "üìå –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 8 features, but ColumnTransformer is expecting 1 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-a3ef53ae9d1c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä –∫ –¥–∞–Ω–Ω—ã–º\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mX_train_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0mX_val_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0;31m# ndarray was used for fitting or transforming, thus we only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;31m# check that n_features_in_ is consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m             \u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2830\u001b[0m             \u001b[0;34mf\"X has {n_features} features, but {estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;34mf\"is expecting {estimator.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 8 features, but ColumnTransformer is expecting 1 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# –ü—É—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/colab_pdf_model\"\n",
        "MODEL_PATH = os.path.join(GDRIVE_PATH, \"model.keras\")  # –£–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ .keras\n",
        "TOKENIZER_PATH = os.path.join(GDRIVE_PATH, \"tokenizer.pkl\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "with open(TOKENIZER_PATH, \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "model = load_model(MODEL_PATH)\n",
        "print(\"–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
        "def save_model(model, path='model.keras'):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏.\"\"\"\n",
        "    model.save(path)\n",
        "    print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}\")\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
        "def generate_text(seed_text, next_words=50, save_after_generation=True):\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (seed_text) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\"\"\"\n",
        "    max_sequence_length = 10  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –Ω—É–∂–Ω—É—é –¥–ª–∏–Ω—É, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "        sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "\n",
        "        # –ü–∞–¥–¥–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã\n",
        "        padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding=\"pre\")\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ\n",
        "        predicted_index = np.argmax(model.predict(padded_sequence), axis=-1)[0]\n",
        "\n",
        "        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–æ –ø–æ –∏–Ω–¥–µ–∫—Å—É\n",
        "        word = None\n",
        "        for w, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                word = w\n",
        "                break\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ –∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é\n",
        "        if word:\n",
        "            seed_text += \" \" + word\n",
        "        else:\n",
        "            break  # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è\n",
        "\n",
        "    if save_after_generation:\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
        "        save_model(model)\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "generated_text = generate_text(\"println\")\n",
        "print(\"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\", generated_text)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .keras (–µ—Å–ª–∏ —ç—Ç–æ –Ω—É–∂–Ω–æ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã)\n",
        "save_model(model, 'model.keras')\n",
        "\n",
        "# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ Google Colab\n",
        "#from google.colab import files\n",
        "#files.download('model.keras')  # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞—à –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8EaU89FbQir",
        "outputId": "25c848ba-2085-4a86-8297-767109594129"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model.keras\n",
            "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: println\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "files.download(MODEL_PATH)\n",
        "\n",
        "# –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "files.download(TOKENIZER_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FUm68y5hbb2z",
        "outputId": "a35d9147-db2a-43c2-e7c5-d78926cfc089"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b1ab2f81-d4b8-47e0-9669-c21a9b36dd36\", \"model.keras\", 144447)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f549c17-6613-4bac-8c62-8749a9f34cc3\", \"tokenizer.pkl\", 532131)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\n",
        "seed_text = \"The quick brown fox jumps\"\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å —É—á—ë—Ç–æ–º OOV —Ç–æ–∫–µ–Ω–∞\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "# –¢—Ä–µ–Ω–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "text_data = [\"The quick brown fox jumps\", \"Over the lazy dog\", \"Hello world\"]  # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "print(\"–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\", sequence)\n",
        "\n",
        "# –ü–∞–¥–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã\n",
        "max_sequence_length = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='pre')\n",
        "print(\"–ü–∞–¥–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\", padded_sequence)\n",
        "\n",
        "# –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞)\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_sequence_length),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "# –î–ª—è —Ç–µ—Å—Ç–∞ –ø—Ä–æ–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å –ø–∞–¥–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é\n",
        "predicted_probs = model.predict(padded_sequence)  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤\n",
        "predicted_index = np.argmax(predicted_probs, axis=-1)[0]  # –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
        "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {predicted_index}\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–æ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞\n",
        "word = None\n",
        "for w, index in tokenizer.word_index.items():\n",
        "    if index == predicted_index:\n",
        "        word = w\n",
        "        break\n",
        "\n",
        "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: {word}\")\n",
        "\n",
        "# –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π\n",
        "def predict_with_temperature(model, padded_sequence, temperature=0.7):\n",
        "    predictions = model.predict(padded_sequence)[0]\n",
        "    predictions = np.log(predictions + 1e-7) / temperature  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º\n",
        "    return np.random.choice(len(predictions), p=predictions)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π\n",
        "predicted_index_temp = predict_with_temperature(model, padded_sequence, temperature=0.7)\n",
        "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π: {predicted_index_temp}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ\n",
        "print(\"\\n–í—Å–µ —Å–ª–æ–≤–∞ –∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ:\")\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    print(f\"{word}: {index}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Na-gQa_t8N",
        "outputId": "869b4854-481a-4bbb-d1cb-b6c086e47b56"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: [[2, 3, 4, 5, 6]]\n",
            "–ü–∞–¥–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: [[0 0 0 0 0 2 3 4 5 6]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: 10\n",
            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: hello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π: 10\n",
            "\n",
            "–í—Å–µ —Å–ª–æ–≤–∞ –∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ:\n",
            "<OOV>: 1\n",
            "the: 2\n",
            "quick: 3\n",
            "brown: 4\n",
            "fox: 5\n",
            "jumps: 6\n",
            "over: 7\n",
            "lazy: 8\n",
            "dog: 9\n",
            "hello: 10\n",
            "world: 11\n"
          ]
        }
      ]
    }
  ]
}