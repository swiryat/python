{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Шаг 1: Загрузка данных\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Предположим, что у вас есть данные в формате CSV\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Признаки\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# Метки\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: data.csv not found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Шаг 1: Загрузка данных\n",
    "# Предположим, что у вас есть данные в формате CSV\n",
    "data = np.genfromtxt('data.csv', delimiter=',', skip_header=1)\n",
    "X = data[:, :-1]  # Признаки\n",
    "y = data[:, -1]   # Метки\n",
    "\n",
    "# Шаг 2: Предварительная обработка данных\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Шаг 3: Кодирование меток\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Шаг 4: Масштабирование функций\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Вот пример базовой программы для подготовки данных для нейронной сети. \n",
    "# Пожалуйста, учтите, что реальные данные и задачи могут требовать более сложной обработки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Предположим, что у нас есть набор данных в формате pandas DataFrame\n",
    "import pandas as pd\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Этап предварительной обработки данных\n",
    "# Разделение данных на признаки (X) и целевую переменную (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Нормализация данных\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Этап обучения модели\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Этап применения модели\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Оценка модели\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Точность модели: {accuracy}')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Определение параметров для настройки\n",
    "parameters = {'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Создание объекта GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring='accuracy', cv=5)\n",
    "\n",
    "# Проведение поиска по сетке для нахождения лучших параметров\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Получение лучших параметров\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "# Обучение модели с использованием лучших параметров\n",
    "model = LogisticRegression(C=best_parameters['C'], penalty=best_parameters['penalty'])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Применение модели и оценка с использованием новых параметров\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Улучшенная точность модели: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Определение URL-адреса для сбора данных\n",
    "url = 'https://example.com/data'\n",
    "\n",
    "# Отправка запроса на сервер\n",
    "response = requests.get(url)\n",
    "\n",
    "# Парсинг HTML-ответа с помощью BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Извлечение данных из HTML\n",
    "data = []\n",
    "for row in soup.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "# Преобразование данных в DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Сохранение данных в CSV-файл\n",
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Шаг 1: Загрузка данных\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data.drop('target_column', axis=1)  # Признаки\n",
    "y = data['target_column']  # Метки\n",
    "\n",
    "# Шаг 2: Предварительная обработка данных\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Шаг 3: Создание пайплайна для обработки данных\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Шаг 4: Применение пайплайна к данным\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Вот улучшенная версия программы для подготовки данных для нейронной сети. \n",
    "# Этот код включает более продвинутые методы обработки данных, такие как обработка пропущенных значений и кодирование категориальных признаков с использованием пайплайнов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Шаг 1: Загрузка данных\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data.drop('target_column', axis=1)  # Признаки\n",
    "y = data['target_column']  # Метки\n",
    "\n",
    "# Шаг 2: Предварительная обработка данных\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Шаг 3: Создание пайплайна для обработки данных\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Шаг 4: Применение пайплайна к данным\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Шаг 5: Преобразование данных обратно в DataFrame (опционально)\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=numeric_features.tolist() + preprocessor.named_transformers_['cat']['onehot'].get_feature_names().tolist())\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=numeric_features.tolist() + preprocessor.named_transformers_['cat']['onehot'].get_feature_names().tolist())\n",
    "\n",
    "# Вот более улучшенная и полная модель на Python для подготовки данных для нейронной сети. \n",
    "# Этот код включает обработку пропущенных значений, кодирование категориальных признаков, масштабирование числовых признаков и использование пайплайнов для обработки данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Шаг 6: Дополнительные шаги\n",
    "# Удаление выбросов с использованием Isolation Forest\n",
    "outlier_detector = IsolationForest(contamination=0.1)  # Пример контаминации\n",
    "outliers = outlier_detector.fit_predict(X_train_processed)\n",
    "X_train_processed_no_outliers = X_train_processed[outliers == 1]\n",
    "y_train_no_outliers = y_train[outliers == 1]\n",
    "\n",
    "# Балансировка классов с использованием SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed_no_outliers, y_train_no_outliers)\n",
    "\n",
    "# Шаг 7: Обучение нейронной сети на обработанных данных\n",
    "# Ваш код обучения нейронной сети здесь\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Шаг 1: Загрузка данных\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data.drop('target_column', axis=1)  # Признаки\n",
    "y = data['target_column']  # Метки\n",
    "\n",
    "# Шаг 2: Предварительная обработка данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Шаг 3: Создание пайплайна для обработки данных\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Применение пайплайна к данным\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Удаление выбросов с использованием Isolation Forest\n",
    "outlier_detector = IsolationForest(contamination=0.1)\n",
    "outliers = outlier_detector.fit_predict(X_train_processed)\n",
    "X_train_processed_no_outliers = X_train_processed[outliers == 1]\n",
    "y_train_no_outliers = y_train[outliers == 1]\n",
    "\n",
    "# Балансировка классов с использованием SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed_no_outliers, y_train_no_outliers)\n",
    "\n",
    "# Преобразование данных обратно в DataFrame (опционально)\n",
    "X_train_processed_df = pd.DataFrame(X_train_balanced, columns=numeric_features.tolist() + preprocessor.named_transformers_['cat']['onehot'].get_feature_names().tolist())\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=numeric_features.tolist() + preprocessor.named_transformers_['cat']['onehot'].get_feature_names().tolist())\n",
    "\n",
    "# Шаг 4: Обучение нейронной сети на обработанных данных\n",
    "# Ваш код обучения нейронной сети здесь\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
